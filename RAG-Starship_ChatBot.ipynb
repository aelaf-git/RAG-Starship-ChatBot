{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your PDF has been split into 31 chunks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Document Loading ---\n",
    "# We'll load the PDF named \"Starship.pdf\"\n",
    "file_path = \"Starship.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "# Load the document. Langchain's loaders return a list of \"Document\" objects.\n",
    "documents = loader.load()\n",
    "\n",
    "# --- 2. Document Chunking ---\n",
    "# Now, we split the loaded documents into smaller chunks.\n",
    "# This is crucial for the RAG model as it helps in finding more specific context.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# The split_documents method will process all our loaded documents.\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Let's check how many chunks we've created\n",
    "print(f\"Your PDF has been split into {len(doc_splits)} chunks.\")\n",
    "\n",
    "# You can inspect a chunk to see its content and metadata\n",
    "# print(doc_splits[5].page_content)\n",
    "# print(doc_splits[5].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AELEF\\AppData\\Local\\Temp\\ipykernel_14700\\2154115945.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n",
      "c:\\Users\\AELEF\\Desktop\\My Co\\RAG-Starship Chatbot\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vector store... This may take a moment.\n",
      "Vector store created successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# --- 3. Indexing and Storage (Creating the Vector Store) ---\n",
    "\n",
    "# We'll use a powerful and popular open-source embedding model from HuggingFace.\n",
    "# The \"all-MiniLM-L6-v2\" model is a great starting point - it's fast and effective.\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'} # Use CPU for embedding generation\n",
    ")\n",
    "\n",
    "# Now we create the FAISS vector store.\n",
    "# This single command does a lot:\n",
    "# 1. It takes our document chunks (doc_splits).\n",
    "# 2. It uses the HuggingFace model to create an embedding for each chunk.\n",
    "# 3. It stores all these embeddings in a FAISS index, ready for searching.\n",
    "print(\"Creating the vector store... This may take a moment.\")\n",
    "vector_store = FAISS.from_documents(doc_splits, embedding_model)\n",
    "print(\"Vector store created successfully!\")\n",
    "\n",
    "# To make our retriever accessible for later use, we can save it locally.\n",
    "# This prevents us from having to re-process the PDF every time we run the notebook.\n",
    "# vector_store.save_local(\"faiss_starship_index\")\n",
    "\n",
    "# To load it back up later, you would use:\n",
    "# vector_store = FAISS.load_local(\"faiss_starship_index\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 4 documents for the query: 'What is the purpose of the Starship's heat shield?'\n",
      "\n",
      "--- Document 1 ---\n",
      "\n",
      "Starship System Starship Spacecraft\n",
      "• Starship's heat shield, composed of thousands of \n",
      "hexagonal black tiles that can withstand temperatures of \n",
      "2,600 °F, is designed to be used many times without \n",
      "maintenance between flights \n",
      "• The tiles are made of silica and are attached with pins \n",
      "rather than glued with small gaps in between to allow \n",
      "for thermal  expansion\n",
      "• Tiles  hexagonal shape facilitate mass production and \n",
      "prevent hot plasma from causing severe damage to the \n",
      "vehicle\n",
      "OLLI Fall 2023 11\n",
      "\n",
      "Source: Starship.pdf, Page: 10\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Document 2 ---\n",
      "\n",
      "Super Heavy Booster and Starship Construction\n",
      "• The stainless steel rings that make up the Starship \n",
      "system’s structure are welded together using laser \n",
      "welding or TIP-TIG welding\n",
      "• Friction-stir welding is used on the aluminum \n",
      "material in the Falcon 9 \n",
      "• Shapes of SS nose one panels has changed on recent \n",
      "prototypes as have configuration of SS rings\n",
      "• The grid fins do not retract and remain extended during \n",
      "ascent\n",
      "• During unpowered flight in the vacuum of space,control \n",
      "authority is provided by cold gas thrusters fed with \n",
      "residual ullage gas\n",
      "• Space between the top of the propellant load and the top of the tank \n",
      "is known as \"ullage space\"\n",
      "• The interstage also has protrudinghardpoints, located \n",
      "between grid fins, allowing the booster to be lifted or \n",
      "caught by the launch tower\n",
      "• All boosters now have an additional 2m tall vented \n",
      "interstage added, as well as a protective dome\n",
      "OLLI Fall 2023 24\n",
      "\n",
      "Source: Starship.pdf, Page: 23\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Document 3 ---\n",
      "\n",
      "Starship System Super Heavy\n",
      "• The booster is equipped with four electrically \n",
      "actuated grid fins  ~ 6,600 lb each \n",
      "• Adjacent  pairs of grid fins are spaced sixty degrees apart\n",
      "• The grid fins do not retract and remain extended during \n",
      "ascent\n",
      "• The booster has can be lifted through \n",
      "protruding hardpoints located between grid fins\n",
      "• The vented interstage is between the booster and the \n",
      "Starship \n",
      "• This enables Starship to use hot staging\n",
      "• The second stage separates when some of the first stages \n",
      "engine are still firing – allowing increased pay load\n",
      "• During unpowered flight in the vacuum of space,control \n",
      "authority is provided by cold gas thrusters fed with \n",
      "residual ullage gas\n",
      "OLLI Fall 2023 7\n",
      "\n",
      "Source: Starship.pdf, Page: 6\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Document 4 ---\n",
      "\n",
      "Starship System Starship Spacecraft\n",
      "• The spacecraft has four body flaps, two forward flaps and \n",
      "two aft flaps ,  to control the spacecraft's orientation and \n",
      "help dissipate energy during atmospheric entry\n",
      "• According to SpaceX, the flaps replace the need for wings \n",
      "or tailplane, reduce the fuel needed for landing, and \n",
      "allow landing at destinations in the Solar System where \n",
      "runways don't exist (Mars ?)\n",
      "• Under the forward flaps, hardpoints are used for lifting \n",
      "and catching the spacecraft via mechanical arms\n",
      "• The flap's hinges are sealed in aero-covers because they \n",
      "would be easily damaged during reentry\n",
      "OLLI Fall 2023 10\n",
      "\n",
      "Source: Starship.pdf, Page: 9\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Retrieval ---\n",
    "\n",
    "# A retriever is an interface that returns documents given an unstructured query.\n",
    "# The most common type of retriever is one that is backed by a vector store.\n",
    "# We can easily convert our vector store into a retriever.\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Let's test it out with a sample query.\n",
    "# The retriever will perform a similarity search in the FAISS index.\n",
    "# It will find the chunks whose embeddings are most similar to the query's embedding.\n",
    "sample_query = \"What is the purpose of the Starship's heat shield?\"\n",
    "relevant_docs = retriever.invoke(sample_query)\n",
    "\n",
    "# Let's see what it found. The output will be a list of Document objects.\n",
    "print(f\"Retrieved {len(relevant_docs)} documents for the query: '{sample_query}'\\n\")\n",
    "\n",
    "# We can inspect the content of the retrieved documents.\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"--- Document {i+1} ---\\n\")\n",
    "    print(doc.page_content)\n",
    "    print(f\"\\nSource: {doc.metadata.get('source', 'N/A')}, Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing Retrieval with Reranker ---\n",
      "Reranked and retrieved 2 documents for the query: 'What is the purpose of the Starship's heat shield?'\n",
      "\n",
      "--- Reranked Document 1 ---\n",
      "\n",
      "Starship System Starship Spacecraft\n",
      "• Starship's heat shield, composed of thousands of \n",
      "hexagonal black tiles that can withstand temperatures of \n",
      "2,600 °F, is designed to be used many times without \n",
      "maintenance between flights \n",
      "• The tiles are made of silica and are attached with pins \n",
      "rather than glued with small gaps in between to allow \n",
      "for thermal  expansion\n",
      "• Tiles  hexagonal shape facilitate mass production and \n",
      "prevent hot plasma from causing severe damage to the \n",
      "vehicle\n",
      "OLLI Fall 2023 11\n",
      "\n",
      "Source: Starship.pdf, Page: 10\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Reranked Document 2 ---\n",
      "\n",
      "Starship System Super Heavy\n",
      "• The booster is equipped with four electrically \n",
      "actuated grid fins  ~ 6,600 lb each \n",
      "• Adjacent  pairs of grid fins are spaced sixty degrees apart\n",
      "• The grid fins do not retract and remain extended during \n",
      "ascent\n",
      "• The booster has can be lifted through \n",
      "protruding hardpoints located between grid fins\n",
      "• The vented interstage is between the booster and the \n",
      "Starship \n",
      "• This enables Starship to use hot staging\n",
      "• The second stage separates when some of the first stages \n",
      "engine are still firing – allowing increased pay load\n",
      "• During unpowered flight in the vacuum of space,control \n",
      "authority is provided by cold gas thrusters fed with \n",
      "residual ullage gas\n",
      "OLLI Fall 2023 7\n",
      "\n",
      "Source: Starship.pdf, Page: 6\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder # <-- CORRECT IMPORT\n",
    "\n",
    "# --- Implement a Reranker ---\n",
    "\n",
    "# We instantiate the CrossEncoder model using LangChain's dedicated wrapper.\n",
    "# This ensures compatibility with the rest of the LangChain ecosystem.\n",
    "cross_encoder_model = HuggingFaceCrossEncoder(\n",
    "    model_name='cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "    model_kwargs={'device': 'cpu'} # Use CPU\n",
    ")\n",
    "\n",
    "# The LangChain reranker wrapper uses our new LangChain-compatible model.\n",
    "# It will re-sort the documents returned by the initial retriever.\n",
    "reranker = CrossEncoderReranker(model=cross_encoder_model, top_n=2)\n",
    "\n",
    "# The ContextualCompressionRetriever remains the same.\n",
    "# It combines the base retriever and the reranker.\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker,\n",
    "    base_retriever=retriever # This is the FAISS retriever from the previous step\n",
    ")\n",
    "\n",
    "# Let's test the reranked retrieval\n",
    "print(\"--- Testing Retrieval with Reranker ---\")\n",
    "sample_query = \"What is the purpose of the Starship's heat shield?\"\n",
    "reranked_docs = compression_retriever.invoke(sample_query)\n",
    "\n",
    "# Let's see the improved results\n",
    "print(f\"Reranked and retrieved {len(reranked_docs)} documents for the query: '{sample_query}'\\n\")\n",
    "\n",
    "for i, doc in enumerate(reranked_docs):\n",
    "    print(f\"--- Reranked Document {i+1} ---\\n\")\n",
    "    print(doc.page_content)\n",
    "    print(f\"\\nSource: {doc.metadata.get('source', 'N/A')}, Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Answer ---\n",
      "The purpose of the Starship's heat shield is to withstand temperatures of 2,600 °F and be used many times without maintenance between flights.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# --- 6. Generation ---\n",
    "\n",
    "# First, we need to define the LLM we'll use for generation.\n",
    "# We'll use Llama 3 running on Groq's fast inference engine.\n",
    "# Make sure your GROQ_API_KEY is set in your .env file.\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=1  # Set to 0 for more deterministic, factual answers\n",
    ")\n",
    "\n",
    "# This template instructs the LLM on how to behave. It defines the input variables\n",
    "# (like \"context\" and \"question\") and the structure of the prompt.\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer about Starship, just say that you don't know.\n",
    "Keep the answer concise and based ONLY on the provided context.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Now, we build the final RAG chain using LangChain Expression Language (LCEL).\n",
    "# This chain will orchestrate the entire process from retrieval to generation.\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"A helper function to format the retrieved documents into a single string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# The RAG chain is defined as follows:\n",
    "# 1. The user's question is passed to the retriever.\n",
    "# 2. The retrieved documents are formatted into a single context string.\n",
    "# 3. The question and context are passed to the prompt.\n",
    "# 4. The formatted prompt is passed to the LLM.\n",
    "# 5. The LLM's output is parsed into a string.\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": compression_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Let's ask our question again, but this time through the full RAG chain.\n",
    "final_answer = rag_chain.invoke(sample_query)\n",
    "\n",
    "# Print the final, LLM-generated answer.\n",
    "print(\"--- Final Answer ---\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG-Starship_ChatBot is ready. Type 'exit' to end the chat.\n",
      "\n",
      "--- Answer ---\n",
      "There is no answer to the question \"hey\" since it is not a specific question about Starship.\n",
      "Thank you for using the Starship ChatBot!\n"
     ]
    }
   ],
   "source": [
    "# --- 7. User Interaction ---\n",
    "\n",
    "# We can create a simple loop to interact with our chatbot.\n",
    "print(\"RAG-Starship_ChatBot is ready. Type 'exit' to end the chat.\")\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"\\nAsk a question about SpaceX's Starship: \")\n",
    "    if user_query.lower() == 'exit':\n",
    "        print(\"Thank you for using the Starship ChatBot!\")\n",
    "        break\n",
    "    \n",
    "    # Get the answer from our RAG chain\n",
    "    answer = rag_chain.invoke(user_query)\n",
    "    \n",
    "    print(\"\\n--- Answer ---\")\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
